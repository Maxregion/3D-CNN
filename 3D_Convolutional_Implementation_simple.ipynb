{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import time\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loading\n",
    "o_data=np.load(\"merge\\\\preprocess_turi_slices_image.npy\")\n",
    "label_data=pd.read_csv(\"merge\\\\preprocess_turi_slices_label.csv\")\n",
    "o_label_data=label_data.cancer.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create iterator for batchs\n",
    "class batch_data_generater:\n",
    "    def __init__(self,X,y,batchsize=10,batch_type=\"random\",split=False):\n",
    "    \n",
    "                                  \n",
    "        self.X=X\n",
    "        self.y=self.label_transfer(y)\n",
    "        self.batchsize=batchsize\n",
    "        self.batch_type=batch_type\n",
    "        self.index=0\n",
    "        if split!=False:\n",
    "            \n",
    "            if type(split) is float:\n",
    "                self.X,self.X_test,self.y,self.y_test=train_test_split(self.X, self.y, test_size=split, random_state=0 )\n",
    "            else:\n",
    "                raise ValueError(\"split must be float\")\n",
    "        if batchsize > len(self.X) or batchsize<0:\n",
    "            \n",
    "            raise ValueError(\"batchsize error\")  \n",
    "    def get_test(self):\n",
    "        return self.X_test.astype(\"float32\"),self.y_test.astype(\"float32\")\n",
    "        \n",
    "                \n",
    "    \n",
    "    def label_transfer(self,y):\n",
    "        new=[]\n",
    "        y_map={}\n",
    "        classes=[]\n",
    "        for class_ in y:\n",
    "            if class_ not in classes:\n",
    "                classes.append(class_)\n",
    "        classes.sort()\n",
    "        if len(classes)<=1:\n",
    "            raise ValueError(\"label error\")\n",
    "        for i in range(len(classes)):\n",
    "            y_map.update({classes[i]:i})\n",
    "        \n",
    "        new_y=np.zeros([len(y),len(y_map)])\n",
    "        for i in range(len(y)):\n",
    "            new_y[i][y_map[y[i]]]=1    \n",
    "        return new_y\n",
    "    \n",
    "    def iter_test(self):\n",
    "        #global index\n",
    "        File_length=len(self.X)\n",
    "        end_file=self.index+self.batchsize\n",
    "        batch_index=[]\n",
    "        if end_file >File_length:\n",
    "            end_file=File_length\n",
    "            for i in range(self.index,end_file):\n",
    "                batch_index.append(i)\n",
    "                \n",
    "            self.index=0\n",
    "            end_file=end_file-File_length\n",
    "            \n",
    "        for i in range(self.index,end_file):\n",
    "                batch_index.append(i)\n",
    "        if end_file == File_length:\n",
    "            end_file=0\n",
    "        self.index=end_file\n",
    "        yield self.X[batch_index],self.y[batch_index]\n",
    "        \n",
    "    def iter_test_random(self):\n",
    "    \n",
    "        batch_index = 0\n",
    "        while True:\n",
    "            # shuffle labels and features\n",
    "            batch_index = np.arange(0, len(self.X))\n",
    "            \n",
    "            np.random.shuffle(batch_index)\n",
    "\n",
    "            shuf_features = self.X[batch_index]\n",
    "            \n",
    "            shuf_labels = self.y[batch_index]\n",
    "            \n",
    "            for batch_idx in range(0, len(self.X), self.batchsize):\n",
    "                \n",
    "                images_batch = shuf_features[batch_idx:batch_idx+self.batchsize]\n",
    "                images_batch = images_batch.astype(\"float32\")\n",
    "                labels_batch = shuf_labels[batch_idx:batch_idx+self.batchsize]\n",
    "\n",
    "                yield images_batch, labels_batch\n",
    "                \n",
    "        \n",
    "    \n",
    "    def get_batch(self):\n",
    "        if self.batch_type==\"step\":\n",
    "            iter_style=self.iter_test()\n",
    "            X,y=next(iter_style)\n",
    "        elif self.batch_type==\"random\":\n",
    "            iter_style=self.iter_test_random()\n",
    "            X,y=next(iter_style)\n",
    "        elif self.batch_type==\"echo\":\n",
    "            X=self.X\n",
    "            y=self.y\n",
    "        else:\n",
    "            raise ValueError(\"batch type error:(step,random,echo)\")\n",
    "        return X,y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-ac264292d54b>:40 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-4-ac264292d54b>:42 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-4-ac264292d54b>:50 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-4-ac264292d54b>:58 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-4-ac264292d54b>:66 in <module>.: histogram_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.histogram. Note that tf.summary.histogram uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on their scope.\n",
      "WARNING:tensorflow:From <ipython-input-4-ac264292d54b>:81 in <module>.: scalar_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.scalar. Note that tf.summary.scalar uses the node name instead of the tag. This means that TensorFlow will automatically de-duplicate summary names based on the scope they are created in. Also, passing a tensor or list of tags to a scalar summary op is no longer supported.\n",
      "WARNING:tensorflow:From <ipython-input-4-ac264292d54b>:85 in <module>.: merge_all_summaries (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge_all.\n",
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda364\\lib\\site-packages\\tensorflow\\python\\ops\\logging_ops.py:264 in merge_all_summaries.: merge_summary (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2016-11-30.\n",
      "Instructions for updating:\n",
      "Please switch to tf.summary.merge.\n"
     ]
    }
   ],
   "source": [
    "#setting_environment\n",
    "\n",
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1})\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    " \n",
    "    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1})\n",
    "    return result\n",
    "\n",
    "\n",
    "def weight_variable(shape,name):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1,name=name)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape,name):\n",
    "    initial = tf.constant(0.1, shape=shape,name=name)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv3d(x, W):\n",
    "    return tf.nn.conv3d(x, W, strides=[1,1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2x2(x):\n",
    "    return tf.nn.max_pool3d(x, ksize=[1,2,2,2,1], strides=[1,2,2,2,1], padding='SAME')\n",
    "def max_pool_2x1x1(x):\n",
    "    return tf.nn.max_pool3d(x, ksize=[1,2,1,1,1], strides=[1,2,1,1,1], padding='SAME')\n",
    "with tf.name_scope('inputs'):\n",
    "    xs = tf.placeholder(tf.float32,[None, 64,32,32], name='x_input') \n",
    "    ys = tf.placeholder(tf.float32,[None, 2],name='y_input')\n",
    "    learning_rate = tf.placeholder(tf.float32,[])\n",
    "    x_image = tf.reshape(xs, [-1,64,32,32, 1])\n",
    "    keep_prob = tf.placeholder(tf.float32,[])\n",
    "with tf.name_scope('conv1_layer'):\n",
    "## conv1 layer ##\n",
    "\n",
    "    W_conv1 = weight_variable([3,3,3,1,16],\"weight1\") # patch 3 x 3 x 3, in 1, out 16 \n",
    "    tf.histogram_summary(\"weight1\", W_conv1)\n",
    "    b_conv1 = bias_variable([16],\"bias1\")\n",
    "    tf.histogram_summary(\"bias1\", b_conv1)\n",
    "    h_conv1 = tf.nn.relu(conv3d(x_image, W_conv1) + b_conv1) \n",
    "    h_pool1 = max_pool_2x2x2(h_conv1) # 32 x 16 x 16 x 16\n",
    "with tf.name_scope('conv2_layer'):\n",
    "    ## conv2 layer ##\n",
    "    W_conv2 = weight_variable([3,3,3, 16, 32],\"weight2\") # patch 3 x 3 x 3, in 16, out 32 \n",
    "    \n",
    "    b_conv2 = bias_variable([32],\"bias2\")\n",
    "    tf.histogram_summary(\"bias2\", b_conv2)\n",
    "    h_conv2 = tf.nn.relu(conv3d(h_pool1, W_conv2) + b_conv2) \n",
    "    h_pool2 = max_pool_2x2x2(h_conv2)   #16 x 8 x 8 x 32\n",
    "with tf.name_scope('conv3_layer'):\n",
    "    ## conv3 layer ##\n",
    "    W_conv3 = weight_variable([3,3,3, 32, 64],\"weight3\") # patch 3 x 3 x 3, in 32, out 64 \n",
    "    \n",
    "    b_conv3 = bias_variable([64],\"bias3\")\n",
    "    tf.histogram_summary(\"bias3\", b_conv3)\n",
    "    h_conv3 = tf.nn.relu(conv3d(h_pool2, W_conv3) + b_conv3) \n",
    "    h_pool3 = max_pool_2x2x2(h_conv3)   # 8 x 4 x 4 x 64\n",
    "with tf.name_scope('fc_layer1'):    \n",
    "## fc1 layer ##\n",
    "    W_fc1 = weight_variable([8*4*4*64, 128],\"fc_weight1\")\n",
    "    \n",
    "    b_fc1 = bias_variable([128],\"fcbias1\")\n",
    "    tf.histogram_summary(\"fcbias1\", b_fc1)\n",
    "    h_pool3_flat = tf.reshape(h_pool3, [-1,8*4*4*64])\n",
    "    h_fc1 = tf.nn.relu(tf.matmul(h_pool3_flat, W_fc1) + b_fc1)\n",
    "    h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "with tf.name_scope('fc_layer2'):    \n",
    "## fc2 layer ##\n",
    "    W_fc2 = weight_variable([128, 2],\"fc_weight2\")\n",
    "    b_fc2 = bias_variable([2],\"fcbias2\")\n",
    "with tf.name_scope('prediction'):      \n",
    "    prediction = tf.nn.sigmoid(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "# the error between prediction and real data\n",
    "with tf.name_scope('loss'):\n",
    "    #loss = tf.reduce_mean(tf.square(tf.sub(prediction,ys)))  \n",
    "    cross_entropy = tf.reduce_mean(-tf.reduce_sum(ys * tf.log(prediction+1e-50)))  \n",
    "    #cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(prediction, ys),name=\"cross_entropy\")\n",
    "    tf.scalar_summary(\"cross_entropy\",cross_entropy)   \n",
    "with tf.name_scope('train'): \n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)#start at 1e-4\n",
    "sess = tf.Session()\n",
    "merge=tf.merge_all_summaries()\n",
    "saver = tf.train.Saver()\n",
    "if int((tf.__version__).split('.')[1]) < 12:\n",
    "    init = tf.initialize_all_variables()\n",
    "else:\n",
    "    init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "if int((tf.__version__).split('.')[1]) < 12:  # tensorflow version < 0.12\n",
    "    writer = tf.train.SummaryWriter('log2', sess.graph)\n",
    "else: # tensorflow version >= 0.12\n",
    "    writer = tf.summary.FileWriter(\"log2\", sess.graph)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#setting btach,lr,saving path,\n",
    "batchData=batch_data_generater(o_data,o_label_data,1,split=0.1)\n",
    "lr=1e-4\n",
    "path_save=\"regre\"\n",
    "\n",
    "#early_start [m,n] run m different start environments, train n steps to get the best start environment\n",
    "early_start=[3,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(early_start[0]):\n",
    "    total_loss=0\n",
    "    for j in range(early_start[1]):\n",
    "        batch_data,batch_label=batchData.get_batch()\n",
    "        _,loss,pred=sess.run([train_step,cross_entropy,prediction], feed_dict={xs: batch_data, ys: batch_label,learning_rate:lr, keep_prob: 0.5})\n",
    "        total_loss+=loss\n",
    "        if i ==0:\n",
    "            loss_sum=total_loss\n",
    "            saver.save(sess, path_save)\n",
    "        else:\n",
    "            if total_loss<loss_sum:\n",
    "                saver.save(sess, path_save)\n",
    "    if int((tf.__version__).split('.')[1]) < 12:\n",
    "        init = tf.initialize_all_variables()\n",
    "    else:\n",
    "        init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load the best start environment\n",
    "saver.restore(sess, path_save)\n",
    "#setting train state\n",
    "#Maximum n steps\n",
    "training_steps=10\n",
    "#check arc after every n steps\n",
    "checking=3\n",
    "#early_stop loss\n",
    "early_stop_loss=0.001\n",
    "#decrease lr after every n steps\n",
    "lr_decrease_steps=2\n",
    "total_loss=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\regre'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(training_steps):\n",
    "    batch_data,batch_label=batchData.get_batch()\n",
    "    _,loss=sess.run([train_step,cross_entropy], feed_dict={xs: batch_data, ys: batch_label,learning_rate:lr, keep_prob: 0.5})\n",
    "    total_loss+=loss\n",
    "    if i%checking==0 and i!=0:\n",
    "       \n",
    "        x_test,y_test=batchData.get_test()\n",
    "        result = sess.run(merge,feed_dict={xs: x_test, ys: y_test,learning_rate:lr,keep_prob:1})\n",
    "        writer.add_summary(result, i)\n",
    "        loss_mean=total_loss/checking\n",
    "        #print(\"accuracy:\",compute_accuracy(x_test, y_test))\n",
    "        if early_stop_loss>loss_mean:\n",
    "            break        \n",
    "    if i%lr_decrease_steps==0 and i!=0:        \n",
    "        lr=lr/2\n",
    "#save model\n",
    "saver.save(sess, path_save)      \n",
    "#out class solution\n",
    "#sense_layer=sess.run([h_fc1], feed_dict={xs: batch_data, ys: batch_label,learning_rate:lr, keep_prob: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testdata=batch_data_generater(o_data,o_label_data,1,batch_type='echo')\n",
    "test_x,test_y=testdata.get_batch()\n",
    "#predict\n",
    "predict=np.asarray(sess.run([prediction], feed_dict={xs:test_x, ys: test_y,learning_rate:lr, keep_prob: 1})).reshape([-1,2]).argmax(1)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
